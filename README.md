Neural Network from Scratch (NumPy) â€“ MNIST Digit Classification
This project demonstrates how to build a neural network from scratch using only NumPy and linear algebra, without relying on deep learning frameworks like TensorFlow or PyTorch. The model is trained to classify handwritten digits from the MNIST dataset.

ğŸ“Œ Features
Fully implemented feedforward neural network using NumPy

Forward propagation & backpropagation implemented from scratch

Uses gradient descent for optimization

Classifies MNIST handwritten digits (0â€“9)

Great for understanding the fundamentals of neural networks

ğŸ› ï¸ Tech Stack
Language: Python

Libraries: NumPy, Matplotlib (for visualization), pickle (for loading MNIST dataset)

ğŸ“¦ Installation
Clone the repository

bash
Copy
Edit
git clone https://github.com/your-username/neural-network-from-scratch.git
cd neural-network-from-scratch
Install dependencies

bash
Copy
Edit
pip install numpy matplotlib
Run the training script

bash
Copy
Edit
python main.py
ğŸ” How It Works
Data Preprocessing â€“ MNIST images are normalized and flattened into vectors.

Network Architecture â€“

Input layer: 784 neurons (28Ã—28 pixels)

Hidden layers: Configurable (e.g., 128, 64 neurons)

Output layer: 10 neurons (digits 0â€“9, softmax activation)

Training â€“ Uses cross-entropy loss & stochastic gradient descent.

Prediction â€“ Outputs the most probable digit for a given image.

ğŸ“Š Results
Achieves ~90â€“95% accuracy on the MNIST test set after sufficient epochs.

Visualizes training loss and accuracy over time.

ğŸ“¸ Sample Output
(Add a screenshot of predicted digits here if possible)

ğŸ“œ License
This project is open-source and free to use for educational purposes.


